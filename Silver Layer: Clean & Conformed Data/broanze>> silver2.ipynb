{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1501fbb6",
   "metadata": {},
   "source": [
    "# ETL: Bronze to Silver Layer Transformation\n",
    "\n",
    "This notebook connects to a SQL Server database, extracts raw data from the `bronze` schema, performs cleaning and transformations, and loads the clean, structured data into the `silver` schema. The process is optimized to handle large tables by training models on subsamples and processing data in memory-safe chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f45db5",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b189f8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully. ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Pandas display options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "print(\"Libraries imported successfully. ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae0811",
   "metadata": {},
   "source": [
    "## 2. Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45acc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to SQL Server established. ✅\n"
     ]
    }
   ],
   "source": [
    "server = \"127.0.0.1,1433\"\n",
    "database = \"Financial Transaction WHD\"\n",
    "username = \"sa\"\n",
    "password = \"Mfa01042004!!\"\n",
    "\n",
    "try:\n",
    "    conn_str = f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "    engine = create_engine(conn_str)\n",
    "    print(\"Connection to SQL Server established. ✅\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to database: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d97ff",
   "metadata": {},
   "source": [
    "## 3. Process Ancillary Tables (Bronze → Silver)\n",
    "\n",
    "These tables are smaller and can be processed in a single pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc1e61a",
   "metadata": {},
   "source": [
    "## 4. Process Main Transactions Table\n",
    "\n",
    "This is the largest table and requires an optimized workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961178f",
   "metadata": {},
   "source": [
    "### Step 4.1: Load and Perform Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff0ad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Transactions Data ---\n",
      "Step 1: Loading raw transactions data...\n",
      "Loaded 1048575 transaction records.\n",
      "Step 2: Performing initial data cleaning...\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Processing Transactions Data ---\")\n",
    "# Step 1: Load raw transaction data from Bronze\n",
    "print(\"Step 1: Loading raw transactions data...\")\n",
    "bronze_txn = pd.read_sql(\"SELECT * FROM bronze.transactions_data\", engine)\n",
    "print(f\"Loaded {len(bronze_txn)} transaction records.\")\n",
    "\n",
    "# Step 2: Perform initial, low-memory cleaning\n",
    "print(\"Step 2: Performing initial data cleaning...\")\n",
    "bronze_txn[\"merchant_state\"] = bronze_txn[\"merchant_state\"].astype(str).str.strip()\n",
    "bronze_txn[\"merchant_city\"] = bronze_txn[\"merchant_city\"].astype(str).str.strip()\n",
    "bronze_txn[\"zip\"] = bronze_txn[\"zip\"].astype(str).str.strip()\n",
    "\n",
    "# Fix cases where zip code was mistakenly entered in the state column\n",
    "mask_zip_from_state = (~bronze_txn[\"zip\"].str.match(r\"^\\d+$\", na=False) & bronze_txn[\"merchant_state\"].str.match(r\"^\\d+$\", na=False))\n",
    "bronze_txn.loc[mask_zip_from_state, \"zip\"] = bronze_txn.loc[mask_zip_from_state, \"merchant_state\"]\n",
    "bronze_txn.loc[mask_zip_from_state, \"merchant_state\"] = np.nan\n",
    "\n",
    "# Invalidate incorrect states and cities\n",
    "bronze_txn.loc[bronze_txn[\"merchant_state\"].str.len() > 2, \"merchant_state\"] = np.nan\n",
    "invalid_city_mask = (bronze_txn[\"merchant_city\"].str.match(r\"^\\d+$\", na=False) | (bronze_txn[\"merchant_city\"].str.len() <= 2))\n",
    "bronze_txn.loc[invalid_city_mask, \"merchant_city\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86bd06",
   "metadata": {},
   "source": [
    "### Step 4.2: Train Models to Impute Missing Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f66a5a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Training location imputation models on a data sample...\n",
      "Training state model on 50000 samples...\n",
      "Training city model on 50000 samples...\n",
      "Models trained successfully. ✅\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Training location imputation models on a data sample...\")\n",
    "# Create a temporary DataFrame with clean, known locations\n",
    "temp_df = bronze_txn[(~bronze_txn[\"merchant_city\"].isna()) & (~bronze_txn[\"merchant_state\"].isna())][[\"merchant_state\", \"merchant_city\", \"zip\"]].copy()\n",
    "temp_df['zip'] = pd.to_numeric(temp_df['zip'], errors='coerce')\n",
    "temp_df.dropna(subset=['zip'], inplace=True)\n",
    "temp_df['zip'] = temp_df['zip'].astype(int)\n",
    "\n",
    "# Encode labels\n",
    "le_state = LabelEncoder()\n",
    "temp_df['state_encoded'] = le_state.fit_transform(temp_df['merchant_state'])\n",
    "le_city = LabelEncoder()\n",
    "temp_df['city_encoded'] = le_city.fit_transform(temp_df['merchant_city'])\n",
    "\n",
    "# Create a smaller sample for training\n",
    "sample_size = min(50000, len(temp_df))\n",
    "train_sample = temp_df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "X_train = train_sample[['zip']]\n",
    "y_state_train = train_sample['state_encoded']\n",
    "y_city_train = train_sample['city_encoded']\n",
    "\n",
    "# Train state model\n",
    "print(f\"Training state model on {len(X_train)} samples...\")\n",
    "state_model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "state_model.fit(X_train, y_state_train)\n",
    "\n",
    "# Train city model\n",
    "print(f\"Training city model on {len(X_train)} samples...\")\n",
    "city_model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "city_model.fit(X_train, y_city_train)\n",
    "\n",
    "print(\"Models trained successfully. ✅\")\n",
    "del temp_df, train_sample, X_train # Clean up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b45aee",
   "metadata": {},
   "source": [
    "### Step 4.3: Process Full Dataset in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edb5eec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Cleaning and imputing full dataset in memory-safe chunks...\n",
      "Processing chunk 1/11...\n",
      "Processing chunk 2/11...\n",
      "Processing chunk 3/11...\n",
      "Processing chunk 4/11...\n",
      "Processing chunk 5/11...\n",
      "Processing chunk 6/11...\n",
      "Processing chunk 7/11...\n",
      "Processing chunk 8/11...\n",
      "Processing chunk 9/11...\n",
      "Processing chunk 10/11...\n",
      "Processing chunk 11/11...\n",
      "Combining all cleaned chunks...\n",
      "Transactions DataFrame fully cleaned and imputed. ✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Step 4: Cleaning and imputing full dataset in memory-safe chunks...\")\n",
    "def clean_amount(val):\n",
    "    if pd.isna(val): return None\n",
    "    val = str(val).replace(\"$\", \"\").replace(\",\", \"\")\n",
    "    if val.startswith(\"(\") and val.endswith(\")\"): return -float(val.strip(\"() \"))\n",
    "    try: return float(val)\n",
    "    except (ValueError, TypeError): return None\n",
    "\n",
    "cleaned_chunks = []\n",
    "chunk_size = 100000\n",
    "num_chunks = int(np.ceil(len(bronze_txn) / chunk_size))\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start_index = i * chunk_size\n",
    "    end_index = start_index + chunk_size\n",
    "    chunk = bronze_txn.iloc[start_index:end_index].copy()\n",
    "    \n",
    "    print(f\"Processing chunk {i+1}/{num_chunks}...\")\n",
    "\n",
    "    # Apply cleaning to the chunk\n",
    "    chunk[\"date\"] = pd.to_datetime(chunk[\"date\"], errors=\"coerce\")\n",
    "    chunk[\"amount\"] = chunk[\"amount\"].apply(clean_amount)\n",
    "    chunk[\"errors\"] = chunk[\"errors\"].fillna(\"None\")\n",
    "    for col in [\"id\", \"client_id\", \"card_id\", \"merchant_id\"]:\n",
    "        chunk[col] = chunk[col].astype(str).str.strip()\n",
    "\n",
    "    # Impute missing locations within the chunk\n",
    "    chunk['zip'] = pd.to_numeric(chunk['zip'], errors='coerce').astype('Int64')\n",
    "    missing_mask = (chunk['merchant_state'].isna() | chunk['merchant_city'].isna()) & chunk['zip'].notna()\n",
    "    rows_to_impute = chunk[missing_mask]\n",
    "\n",
    "    if not rows_to_impute.empty:\n",
    "        predicted_states = le_state.inverse_transform(state_model.predict(rows_to_impute[['zip']]))\n",
    "        predicted_cities = le_city.inverse_transform(city_model.predict(rows_to_impute[['zip']]))\n",
    "        chunk.loc[missing_mask, 'merchant_state'] = predicted_states\n",
    "        chunk.loc[missing_mask, 'merchant_city'] = predicted_cities\n",
    "\n",
    "    cleaned_chunks.append(chunk)\n",
    "\n",
    "# Combine chunks into the final DataFrame\n",
    "print(\"Combining all cleaned chunks...\")\n",
    "txn_final = pd.concat(cleaned_chunks, ignore_index=True)\n",
    "print(\"Transactions DataFrame fully cleaned and imputed. ✅\")\n",
    "\n",
    "# Clean up memory\n",
    "del cleaned_chunks, bronze_txn\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53808aa",
   "metadata": {},
   "source": [
    "### Step 4.4: Load Final Data and Close Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0797002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions data saved successfully to /home/m.farrag/transactions_data.csv\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/home/m.farrag/transactions_data.csv\"\n",
    "\n",
    "txn_final.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "print(f\"Transactions data saved successfully to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600eff9-9d18-4abc-b201-ae3a868c477c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
